{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMHob0gNfzO/y3idueNK4dQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codeyumm/adl-midterm-project/blob/main/Group_02_Midterm_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "K2SPt3PBXpZJ"
      },
      "outputs": [],
      "source": [
        "# libs\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set random seed for reproducibility\n",
        "np.random.seed(66)\n",
        "\n",
        "torch.manual_seed(66)\n",
        "\n",
        "# select deive, CPU or GPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"Using\", device, \"for computation.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4qo8USKDAaC",
        "outputId": "847141a7-67ab-4287-e4e9-e4ae8657be06"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu for computation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the dataset\n",
        "train_data = datasets.KMNIST(root=\"./data\", train=True, download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JI3ilXzwX32a",
        "outputId": "6137c94c-18d9-436a-83c5-36d9f6f9465f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 18.2M/18.2M [00:05<00:00, 3.23MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 768kB/s]\n",
            "100%|██████████| 3.04M/3.04M [00:00<00:00, 5.03MB/s]\n",
            "100%|██████████| 5.12k/5.12k [00:00<00:00, 6.62MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observing the dataset"
      ],
      "metadata": {
        "id": "bCtC_u_Euati"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# shape of dataset\n",
        "print(f\"Number of training samples : {len(train_data)}\")"
      ],
      "metadata": {
        "id": "NL-LIO3YYdTK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b262ec7e-01a3-430e-d331-8102c12a2ef1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples : 60000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# display sample image from the test dataset\n",
        "image, label = train_data[6]\n",
        "\n",
        "# size of image\n",
        "print(f\"Size of image is : {image.size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxUKdI_pudHp",
        "outputId": "2d8f6630-f974-4b49-9084-5066d8c232ae"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of image is : (28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We can see that we have 28 x 28 image and it is a grayscale as there is only one color channel"
      ],
      "metadata": {
        "id": "a0mT7YXYvKWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sample image\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.title(f\"Label: {label}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "40XjpI1Pu7KZ",
        "outputId": "60c02a94-c1d3-44e0-ee95-8aa18a014df4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now we need to normalize the dataset for better computation and also we need to convert it to tensors becasue our deep learning models works with tensors"
      ],
      "metadata": {
        "id": "e9LAK1hhv1kh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset\n",
        "train_data = datasets.KMNIST(root=\"./data\", train=True, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "\n",
        "data_loader = torch.utils.data.DataLoader(train_data, batch_size=len(train_data), shuffle=False)\n",
        "\n",
        "# get all pixel values\n",
        "data = next(iter(data_loader))[0]\n",
        "mean = data.mean()\n",
        "std = data.std()\n",
        "\n",
        "print(f\"Mean of train dataset : {mean}\")\n",
        "print(f\"STD of train dataset : {std}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGC8X3r0v0s6",
        "outputId": "2864302d-31ba-4a69-b69c-eecbdccd832d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean of train dataset : 0.19176216423511505\n",
            "STD of train dataset : 0.3483428359031677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# transforms object\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((mean,), (std,))\n",
        "])\n",
        "\n",
        "# load the dataset with transforms object to get the normalize data\n",
        "\n",
        "train_data = datasets.KMNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_data = datasets.KMNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "\n",
        "# shape of dataset\n",
        "print(f\"Number of training samples : {len(train_data)}\")\n",
        "print(f\"Number of testing samples : {len(test_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZlJbQhiu7yK",
        "outputId": "5f08d178-47a9-478d-a316-8df9b4685150"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples : 60000\n",
            "Number of testing samples : 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### At this point, our input is standardize in terms of we have normalized it wiht mean and std. Now all the pixels are in range of [-1, 1] and most of them should be centerd around 0"
      ],
      "metadata": {
        "id": "57pbuLBj0JJG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the model architecture"
      ],
      "metadata": {
        "id": "89UhjVks12jE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model network\n",
        "\n",
        "class Kmnist_Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Kmnist_Net, self).__init__()\n",
        "\n",
        "    # convert 2d (28 * 28) to 1d 784)\n",
        "\n",
        "    self.flatten = nn.Flatten()\n",
        "\n",
        "    # first fully connected layer input(784) to hidden 1 (128)\n",
        "    self.fc1 = nn.Linear(784, 128)\n",
        "\n",
        "    # second fully connected layer input(128) to hidden 2 (64)\n",
        "    self.fc2 = nn.Linear(128, 64)\n",
        "\n",
        "    # third fully connected layer input(64) to output (10)\n",
        "    self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    # activation function - for hidden layers - ReLu\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    # activation fucntion - for output layer - SoftMax\n",
        "\n",
        "    # self.soft_max = nn.Softmax()\n",
        "    # we dont need to add this here because crossentropyloss,\n",
        "    # expects raw logits as input and it applies logsoftmax\n",
        "\n",
        "\n",
        "  # forward pass\n",
        "  def forward(self, input):\n",
        "\n",
        "    # 2d to 1d\n",
        "    x = self.flatten(input)\n",
        "\n",
        "    # pass it to fc1 with ReLu activation\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    # pass it to fc2 with ReLu activation\n",
        "    x = self.fc2(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    # pass it to fc3 with Softmax activation\n",
        "    x = self.fc3(x)\n",
        "    # x = self.soft_max(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dXB1JKAo12WN"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In above model we are not adding softmax activation function becasue, CrossEntropyLoss computes the cross entropy loss between input logits and target and it also applies  LogSoftmax internally\n",
        "\n",
        "### Source - [CrossEntropyLoss](https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)"
      ],
      "metadata": {
        "id": "KuqRdFKF88kO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining Inital Hyperparameters"
      ],
      "metadata": {
        "id": "YMbvkfAl_IXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# how fast we want to move on gradient to find minima\n",
        "learning_rate = 0.001\n",
        "\n",
        "# number of time we want to go thorugh whole dataset\n",
        "epochs = 50\n",
        "\n",
        "# number of samples the model sees before updating weights\n",
        "batch_size = 64\n",
        "\n",
        "# define loss function - CrossEntropyLoss()\n",
        "loss_function = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "nIbk9R9O9ptb"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data loader object to get data into batch\n",
        "# shuffle True for tranloader becasue it prevents model to remeber the pattern of data\n",
        "train_loader = DataLoader(train_data, batch_size = batch_size, shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(test_data, batch_size= 1000, shuffle=False)"
      ],
      "metadata": {
        "id": "hXUPtiriBdTA"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Traning Loooooop"
      ],
      "metadata": {
        "id": "VrV1qKI4BQiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(loss_function, optimizer = None):\n",
        "\n",
        "  # move model to gpu\n",
        "  model = Kmnist_Net().to(device)\n",
        "\n",
        "  # set model to train\n",
        "  model.train()\n",
        "\n",
        "\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # get the output\n",
        "      output = model(images)\n",
        "\n",
        "      # find the loss\n",
        "      loss = loss_function(output, labels)\n",
        "\n",
        "      # backpropogate to find optimal weights\n",
        "      loss.backward()\n",
        "\n",
        "      # update weight\n",
        "      optimizer.step()\n",
        "\n",
        "      # calculate the loss\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      # find correct output\n",
        "      _, predicted = torch.max(output, 1)\n",
        "\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "      # total images seen by model\n",
        "      total += labels.size(0)\n",
        "\n",
        "    train_acc = ( correct / total ) * 100\n",
        "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {running_loss:.4f},  Train Accuracy : {train_acc:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Yy9XwrFmAlIk"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Loop"
      ],
      "metadata": {
        "id": "rZBzxjIsKVf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Evaluate(model):\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  # set model to eval mode\n",
        "  model.eval()\n",
        "\n",
        "  # disavle gradient calculations\n",
        "  torch.no_grad()\n",
        "  with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # get the output\n",
        "        output = model(images)\n",
        "\n",
        "        # get the predicted labels\n",
        "        _, predicted = torch.max(output, 1)\n",
        "\n",
        "        # total number of images\n",
        "        total += labels.size(0)\n",
        "\n",
        "        # count correct predictions\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "\n",
        "  test_accuracy = 100 * (correct / total)\n",
        "  print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qt_jpk2WGITw"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9iizq6SJLMS9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}